import sys
import re
import pandas as pd
import requests
from bs4 import BeautifulSoup

# å¼•æ•°ã§ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å—ã‘å–ã‚‹
if len(sys.argv) > 1:
    input_path = sys.argv[1]
else:
    input_path = "ä¼æ¥­æƒ…å ±_cleaned.xlsx"  # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆç”¨

output_path = input_path.replace(".xlsx", "_æŠ½å‡ºçµæœ_v4.xlsx")

# æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
keywords = ["ä¼šç¤¾æ¦‚è¦", "ä¼æ¥­æƒ…å ±", "ä¼šç¤¾æ¡ˆå†…", "ä¼æ¥­æ¦‚è¦", "ä¼šç¤¾ç´¹ä»‹"]

# æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³
patterns = {
    "å¾“æ¥­å“¡æ•°": re.compile(r"(å¾“æ¥­å“¡æ•°|ç¤¾å“¡æ•°)[\s:ï¼š]*([\d,]+)[\s]*äºº?"),
    "è³‡æœ¬é‡‘": re.compile(r"(è³‡æœ¬é‡‘)[\s:ï¼š]*([0-9,å„„ä¸‡å††]+)"),
    "å£²ä¸Šé«˜": re.compile(r"(å£²ä¸Šé«˜)[\s:ï¼š]*([0-9,å„„å…†ä¸‡å††]+)")
}

# HTMLå–å¾—
def get_html(url):
    try:
        res = requests.get(url, timeout=10, verify=False)
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, "lxml")
        return soup
    except Exception as e:
        print(f"âš ï¸ HTMLå–å¾—å¤±æ•—: {e}")
        return None

# æ¦‚è¦ãƒšãƒ¼ã‚¸æ¢ç´¢
def find_overview_url(soup, base_url):
    links = soup.find_all("a", href=True)
    for link in links:
        href = link["href"]
        text = link.get_text(strip=True)
        if any(kw in text for kw in keywords):
            if href.startswith("http"):
                return href
            elif href.startswith("/"):
                return base_url.rstrip("/") + href
            else:
                return base_url.rstrip("/") + "/" + href
    return None

# Excelèª­ã¿è¾¼ã¿
df = pd.read_excel(input_path)
results = []

# å„URLã”ã¨ã«å‡¦ç†
for index, row in df.iterrows():
    company = row["ä¼æ¥­å"]
    url = row["URL"]
    print(f"\nâ–¶ï¸ å‡¦ç†ä¸­: {company} | {url}")

    if not isinstance(url, str) or not url.startswith("http"):
        print("âš ï¸ URLå½¢å¼ã‚¨ãƒ©ãƒ¼")
        results.append({
            "ä¼æ¥­å": company, "URL": url,
            "å¾“æ¥­å“¡æ•°": "", "è³‡æœ¬é‡‘": "", "å£²ä¸Šé«˜": "",
            "å–å¾—å…ƒ": "ã‚¨ãƒ©ãƒ¼"
        })
        continue

    soup = get_html(url)
    if not soup:
        results.append({
            "ä¼æ¥­å": company, "URL": url,
            "å¾“æ¥­å“¡æ•°": "", "è³‡æœ¬é‡‘": "", "å£²ä¸Šé«˜": "",
            "å–å¾—å…ƒ": "ã‚¨ãƒ©ãƒ¼"
        })
        continue

    overview_url = find_overview_url(soup, url)
    if overview_url and overview_url != url:
        print(f"ğŸ”„ æ¦‚è¦ãƒšãƒ¼ã‚¸é·ç§»: {overview_url}")
        soup = get_html(overview_url)
        if not soup:
            results.append({
                "ä¼æ¥­å": company, "URL": url,
                "å¾“æ¥­å“¡æ•°": "", "è³‡æœ¬é‡‘": "", "å£²ä¸Šé«˜": "",
                "å–å¾—å…ƒ": "ã‚¨ãƒ©ãƒ¼"
            })
            continue

    # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
    text = soup.get_text(separator="\n")

    emp = patterns["å¾“æ¥­å“¡æ•°"].search(text)
    cap = patterns["è³‡æœ¬é‡‘"].search(text)
    rev = patterns["å£²ä¸Šé«˜"].search(text)

    results.append({
        "ä¼æ¥­å": company,
        "URL": url,
        "å¾“æ¥­å“¡æ•°": emp.group(2) + "äºº" if emp else "",
        "è³‡æœ¬é‡‘": cap.group(2) if cap else "",
        "å£²ä¸Šé«˜": rev.group(2) if rev else "",
        "å–å¾—å…ƒ": "HTML"
    })

# å‡ºåŠ›
df_out = pd.DataFrame(results)
df_out.to_excel(output_path, index=False)
print(f"\nâœ… å®Œäº†ï¼ä¿å­˜å…ˆ: {output_path}")
