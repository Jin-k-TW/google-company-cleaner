from bs4 import BeautifulSoup
import requests
import pandas as pd
import re
import os

# å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆExcelãŒã‚ã‚‹å ´æ‰€ã¨åŒã˜ã«ï¼‰
input_path = os.path.join(os.path.dirname(__file__), "ä¼æ¥­æƒ…å ±_cleaned.xlsx")

# æŠ½å‡ºç”¨ã®æ­£è¦è¡¨ç¾
patterns = {
    "å¾“æ¥­å“¡æ•°": re.compile(r"(å¾“æ¥­å“¡æ•°|ç¤¾å“¡æ•°)[^\d]{0,5}(\d{1,4}(?:[,ï¼Œ]?\d{3})*)\s*äºº?"),
    "è³‡æœ¬é‡‘": re.compile(r"(è³‡æœ¬é‡‘)[^\d]{0,5}([\d,ï¼Œ\.]+.*?å††)"),
    "å£²ä¸Šé«˜": re.compile(r"(å£²ä¸Šé«˜)[^\d]{0,5}([\d,ï¼Œ\.]+.*?å††)")
}

# æ¤œç´¢å„ªå…ˆãƒšãƒ¼ã‚¸å
priority_keywords = ["ä¼šç¤¾æ¦‚è¦", "ä¼æ¥­æƒ…å ±", "ä¼šç¤¾æ¡ˆå†…", "ä¼æ¥­æ¦‚è¦", "ä¼šç¤¾ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«"]

results = []

df = pd.read_excel(input_path)

for index, row in df.iterrows():
    company = row["ä¼æ¥­å"]
    url = row["Webã‚µã‚¤ãƒˆURL"]
    print(f"â–¶ï¸ å‡¦ç†ä¸­: {company} | {url}")

    emp = cap = rev = None
    source = "ã‚¨ãƒ©ãƒ¼"

    try:
        if pd.isna(url) or not str(url).startswith("http"):
            raise ValueError("URLãŒç„¡åŠ¹")

        # åˆæœŸãƒšãƒ¼ã‚¸å–å¾—
        res = requests.get(url, timeout=10)
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, "lxml")

        # ä¼šç¤¾æ¦‚è¦ãƒšãƒ¼ã‚¸æ¢ç´¢
        overview_url = None
        for link in soup.find_all("a", href=True):
            link_text = str(link.get_text())
            href = link["href"]
            if any(k in link_text for k in priority_keywords):
                overview_url = requests.compat.urljoin(url, href)
                print(f"ğŸ”— æ¦‚è¦ãƒšãƒ¼ã‚¸ç™ºè¦‹: {overview_url}")
                break

        # è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‚’ä½¿ã†
        if not overview_url:
            overview_url = url

        res = requests.get(overview_url, timeout=10)
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, "lxml")
        text = soup.get_text(separator="\n")

        emp_match = patterns["å¾“æ¥­å“¡æ•°"].search(text)
        cap_match = patterns["è³‡æœ¬é‡‘"].search(text)
        rev_match = patterns["å£²ä¸Šé«˜"].search(text)

        if emp_match or cap_match or rev_match:
            source = "HTML"
            print("âœ… æƒ…å ±æŠ½å‡ºæˆåŠŸ:", end=" ")
            if emp_match: print("å¾“æ¥­å“¡æ•°", end=" ")
            if cap_match: print("è³‡æœ¬é‡‘", end=" ")
            if rev_match: print("å£²ä¸Šé«˜", end=" ")
            print()
        else:
            print("âš ï¸ è©²å½“æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        results.append({
            "ä¼æ¥­å": company,
            "URL": url,
            "å¾“æ¥­å“¡æ•°": emp_match.group(2) + "äºº" if emp_match else "",
            "è³‡æœ¬é‡‘": cap_match.group(2) if cap_match else "",
            "å£²ä¸Šé«˜": rev_match.group(2) if rev_match else "",
            "å–å¾—å…ƒ": source
        })

    except Exception as e:
        print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        results.append({
            "ä¼æ¥­å": company,
            "URL": url,
            "å¾“æ¥­å“¡æ•°": "",
            "è³‡æœ¬é‡‘": "",
            "å£²ä¸Šé«˜": "",
            "å–å¾—å…ƒ": "ã‚¨ãƒ©ãƒ¼"
        })

# ä¿å­˜
output_path = os.path.join(os.path.dirname(__file__), "ä¼æ¥­æƒ…å ±_æŠ½å‡ºçµæœ_v4.xlsx")
df_out = pd.DataFrame(results)
df_out.to_excel(output_path, index=False)
print(f"âœ… å®Œäº†ï¼ä¿å­˜å…ˆ: {output_path}")